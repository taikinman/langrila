{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"NONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure your environmental variables and dependencies are ready to use LLM services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env_api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In langrila, the module to chat with LLM and the module to call tools are completely separated. FunctionalChat class is the combination of those two.\n",
    "\n",
    "- `XXXChatModule`: Only focuses on doing conversation with LLM\n",
    "- `XXXFunctionCallingModule`: Only focuses on calling tools\n",
    "- `XXXFunctionalChat`: The combination of the two. FunctionCallingModule works at first and then ChatModule performs. If any tool is not provided, this module behaves as just ChatModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila.gemini import GeminiFunctionalChat\n",
    "from langrila.openai import OpenAIFunctionalChat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Chat Completion and Gemini support json mode generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For OpenAI Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_openai = OpenAIFunctionalChat(api_key_env_name=\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"# INSTRUCTION\n",
    "Extract fruit names from the following text and output with the following JSON format. Don't generate JSON code block.\n",
    "\n",
    "# TEXT\n",
    "{prompt}\n",
    "\n",
    "# OUTPUT JSON FORMAT\n",
    "{{\n",
    "    \"fruits\": [\"xxx\", \"xxx\", \"xxx\"]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fruits': ['apples', 'oranges', 'bananas']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"This store sells apples, oranges, and bananas.\"\n",
    "prompt = prompt_template.format(prompt=prompt)\n",
    "response = chat_openai.run(prompt=prompt, model_name=\"gpt-4o-2024-08-06\", json_mode=True)\n",
    "json.loads(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pydantic schema is also available with 2 arguments: `json_mode` and `response_schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_openai = OpenAIFunctionalChat(api_key_env_name=\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MathReasoning(steps=[Step(explanation='Start by isolating the term with the variable, 8x, on one side of the equation. To do this, subtract 7 from both sides of the equation.', output='8x + 7 - 7 = -23 - 7'), Step(explanation='Simplify both sides of the equation. On the left side, 7 - 7 cancels out, leaving 8x. On the right side, -23 - 7 equals -30.', output='8x = -30'), Step(explanation='To solve for x, divide both sides of the equation by 8 to get x by itself.', output='(8x)/8 = (-30)/8'), Step(explanation='Simplify the right-hand side. -30 divided by 8 simplifies to -15/4 or -3.75.', output='x = -3.75')], final_answer='x = -3.75')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"how can I solve 8x + 7 = -23\"\n",
    "\n",
    "response = chat_openai.run(\n",
    "    prompt=prompt,\n",
    "    model_name=\"gpt-4o-2024-08-06\",\n",
    "    json_mode=True,  # also needed\n",
    "    response_schema=MathReasoning,\n",
    "    system_instruction=\"You are a helpful math tutor. Guide the user through the solution step by step.\",\n",
    ")\n",
    "\n",
    "# resposne message is in JSON string that can be converted to specified schema\n",
    "resposne_json = json.loads(response.message.content[0].text)\n",
    "MathReasoning(**resposne_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class ResponseSchema(TypedDict):\n",
    "    fruits: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = GeminiFunctionalChat(api_key_env_name=\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PydanticDeprecatedSince20]:The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n"
     ]
    }
   ],
   "source": [
    "prompt = \"This store sells apples, oranges, and bananas.\"\n",
    "\n",
    "response = gemini.run(\n",
    "    prompt=prompt,\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    json_mode=True,\n",
    "    response_schema=ResponseSchema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fruits': ['apples', 'oranges', 'bananas']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
