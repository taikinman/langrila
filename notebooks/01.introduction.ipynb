{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"NONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure your environmental variables and dependencies are ready to use LLM services. Name of the environmental variables is arbitraray because langrila modules accepts that name as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env_api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main modules are `{Provider}Client` module and `Agent` module. `{Provider}Client` module is inherited by each module for a specific llm provider like OpenAI, Google or Anthropic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import Agent\n",
    "from langrila.anthropic import AnthropicClient\n",
    "from langrila.aws import BedrockClient\n",
    "from langrila.google import GoogleClient\n",
    "from langrila.openai import OpenAIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating client modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenAI\n",
    "openai_client = OpenAIClient(api_key_env_name=\"OPENAI_API_KEY\")\n",
    "\n",
    "# For Azure OpenAI\n",
    "azure_openai_client = OpenAIClient(\n",
    "    api_key_env_name=\"AZURE_API_KEY\",\n",
    "    api_type=\"azure\",\n",
    "    azure_api_version=\"2024-11-01-preview\",\n",
    "    azure_endpoint_env_name=\"AZURE_ENDPOINT\",\n",
    "    azure_deployment_env_name=\"AZURE_DEPLOYMENT_ID\",\n",
    ")\n",
    "\n",
    "# For Gemini on Google AI Studio\n",
    "google_dev_client = GoogleClient(api_key_env_name=\"GEMINI_API_KEY\")\n",
    "\n",
    "# For Gemini on Google Cloud VertexAI\n",
    "vertexai_client = GoogleClient(\n",
    "    api_type=\"vertexai\",\n",
    "    project_id_env_name=\"GOOGLE_CLOUD_PROJECT\",\n",
    "    location=\"us-central1\",\n",
    ")\n",
    "\n",
    "# For Claude of Anthropic\n",
    "anthropic_client = AnthropicClient(api_key_env_name=\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# For Claude of Amazon Bedrock\n",
    "claude_bedrock_client = AnthropicClient(\n",
    "    api_type=\"bedrock\",\n",
    "    aws_access_key_env_name=\"AWS_ACCESS_KEY\",\n",
    "    aws_secret_key_env_name=\"AWS_SECRET_KEY\",\n",
    "    aws_region=\"us-east-1\",\n",
    ")\n",
    "\n",
    "# Converse API for Amazon Bedrock\n",
    "bedrock_client = BedrockClient(\n",
    "    region_name=\"us-east-1\",\n",
    "    aws_access_key_env_name=\"AWS_ACCESS_KEY\",\n",
    "    aws_secret_key_env_name=\"AWS_SECRET_KEY\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass any client instance to `Agent`. Almost all arguments is the same as the provider's sdk. Arguments of the `Agent` class are finally passed to the `{Provider}Client` module almost as it is, and `{Provider}Client` is just simple wrapper module of each LLM provider API. So please look at the API reference of each provider API if you want to know what arguments can be accepted by `Agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "azure_openai_agent = Agent(\n",
    "    client=azure_openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "google_agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "vertexai_agent = Agent(\n",
    "    client=vertexai_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "claude_agent = Agent(\n",
    "    client=anthropic_client,\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "claude_bedrock_agent = Agent(\n",
    "    client=claude_bedrock_client,\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "bedrock_agent = Agent(\n",
    "    client=bedrock_client,\n",
    "    modelId=\"us.amazon.nova-lite-v1:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can input your prompt in 2 ways; input string prompt directly or create `Prompt` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import Prompt, TextPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple string prompt\n",
    "prompt1 = \"Hello. How are you today?\"\n",
    "\n",
    "# 2. Prompt instance with raw string\n",
    "prompt2 = Prompt(\n",
    "    role=\"user\",\n",
    "    contents=\"Hello. How are you today?\",\n",
    ")\n",
    "\n",
    "# 3. Prompt instance with list of TextPrompt\n",
    "prompt3 = Prompt(\n",
    "    role=\"user\",\n",
    "    contents=[\n",
    "        TextPrompt(\n",
    "            text=\"Hello. How are you today?\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pattern 2, string specified as `contents` argument is automatically converted to list of `TextPrompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2 == prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(type='Prompt', role='user', contents=[TextPrompt(text='Hello. How are you today?')], name=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(type='Prompt', role='user', contents=[TextPrompt(text='Hello. How are you today?')], name=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous, non-streaming text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:39]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:39]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:41]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = openai_agent.generate_text(prompt=prompt1)\n",
    "\n",
    "# Same as\n",
    "# response = openai_agent.generate_text(prompt=prompt2)\n",
    "# response = openai_agent.generate_text(prompt=prompt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response object is a pydantic model contains formatted response from the provider API, usage, and raw response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bff78c9d0>, raw=ChatCompletion(id='chatcmpl-ApPZMWkw8rPWUQyqyarq6qdf3hbtv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736816380, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_72ed7ab54c', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent class accumulates usage across all API call and interaction with subagent. Sub-agent has its own name, and root agent is named \"root\". You can access this accumulated usage via `response.usage`. `response.usage` is dict-like object, and it has `items()` method and `__getitem__()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('root', Usage(model_name='gpt-4o-mini-2024-07-18', prompt_tokens=14, output_tokens=30))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(model_name='gpt-4o-mini-2024-07-18', prompt_tokens=14, output_tokens=30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage[\"root\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access raw response from the API that is helpful fo9r debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-ApPZMWkw8rPWUQyqyarq6qdf3hbtv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736816380, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_72ed7ab54c', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as other client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:42]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:42]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:43]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bff78fc10>, raw=ChatCompletion(id='chatcmpl-ApPZOdgO2K8xWx4bBA4BsButU7KkF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1736816382, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_65792305e4', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=None, prompt_tokens_details=None), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = azure_openai_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-ApPZOdgO2K8xWx4bBA4BsButU7KkF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1736816382, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_65792305e4', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=None, prompt_tokens_details=None), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on Google AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:43]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:43]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:45]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bff0fe8c0>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = google_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:45]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:45]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:47]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7c002fba00>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = vertexai_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude of Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:47]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:47]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:49]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bff0ff2b0>, raw=Message(id='msg_01TY7BUXPMqaVoZ3HAnFWwBp', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=43, cache_creation_input_tokens=0, cache_read_input_tokens=0)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01TY7BUXPMqaVoZ3HAnFWwBp', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=43, cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:49]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:49]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:51]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7c1bfeed70>, raw=Message(id='msg_bdrk_011YqoEwaaP6iqa4zYJsiGVe', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=45)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_bedrock_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_bdrk_011YqoEwaaP6iqa4zYJsiGVe', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=45))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converse API model such as Amazon Nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:51]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:51]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:52]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need support with something, feel free to let me know.\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need support with something, feel free to let me know.\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfeba7fa0>, raw={'ResponseMetadata': {'RequestId': '8150b086-9690-4a64-b402-c92afe42a3a9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 14 Jan 2025 00:59:52 GMT', 'content-type': 'application/json', 'content-length': '336', 'connection': 'keep-alive', 'x-amzn-requestid': '8150b086-9690-4a64-b402-c92afe42a3a9'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': \"Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need support with something, feel free to let me know.\"}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 7, 'outputTokens': 38, 'totalTokens': 45}, 'metrics': {'latencyMs': 455}}, name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = bedrock_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '8150b086-9690-4a64-b402-c92afe42a3a9',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Tue, 14 Jan 2025 00:59:52 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '336',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '8150b086-9690-4a64-b402-c92afe42a3a9'},\n",
       "  'RetryAttempts': 0},\n",
       " 'output': {'message': {'role': 'assistant',\n",
       "   'content': [{'text': \"Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need support with something, feel free to let me know.\"}]}},\n",
       " 'stopReason': 'end_turn',\n",
       " 'usage': {'inputTokens': 7, 'outputTokens': 38, 'totalTokens': 45},\n",
       " 'metrics': {'latencyMs': 455}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor params vs generation params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except a few parameters, almost all the parameters can be specified in both constructor and generation method (such as `generate_text()` method). Constructor params in `Agent` class can be overriden by the arguments in `generate_text` method (as well as `stream_text(_async)` method as shown later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:53]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello!')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:53]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:54]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Hello there! How can I help you today?')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='Hello there! How can I help you today?')], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfeba7d00>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello there! How can I help you today?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=11, prompt_token_count=3, total_token_count=14), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    temperature=0.0,\n",
    "    max_output_tokens=500,\n",
    ")\n",
    "\n",
    "agent.generate_text(prompt=\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is the same as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:54]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello!')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:54]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:55]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Hello there! How can I help you today?')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='Hello there! How can I help you today?')], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfeba7880>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello there! How can I help you today?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=11, prompt_token_count=3, total_token_count=14), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    ")\n",
    "\n",
    "agent.generate_text(\n",
    "    prompt=\"Hello!\", model=\"gemini-2.0-flash-exp\", temperature=0.0, max_output_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing arguments when initiating is useful to reuse same parameters at multiple place. Especiall, multi-agent system invokes multiple agents at the same time, so it requires many parameters to be static. On the other hand, passing arguments when generating helps you control settings whenever you call the API. This is helpful for the single agent case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, here is an example to specify the same parameter when initializing agent and generating response. (As I mentioned, the later parameter is prioritized.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    n=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:55]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello! How are you?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:55]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:56]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\"), TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=\"Hello! How are you?\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\"),\n",
       " TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `n` parameter is specified in both constructor and `generate_text` method, but `n=2` is used, which was specified for `generate_text` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous, non-streaming text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate_text_async` method allows you to received text asyncronously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:56]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:56]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:57]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfebb2770>, raw=ChatCompletion(id='chatcmpl-ApPZdlYcDa3zaoiMhqMVbjY8JNya8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736816397, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_bd83329f63', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await openai_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:57]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:57]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:59]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bff94f850>, raw=ChatCompletion(id='chatcmpl-ApPZeh9n2ZNYTCFveamTamHosvcWP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1736816398, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_f3927aa00d', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await azure_openai_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 09:59:59]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 09:59:59]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:00]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bff0fc7f0>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await google_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:00]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:00]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:02]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bff9a2230>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await vertexai_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude of Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:02]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:02]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:03]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfebec8e0>, raw=Message(id='msg_01AUALzZJsryUg9npp9ADBiW', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=43, cache_creation_input_tokens=0, cache_read_input_tokens=0)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await claude_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude of Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:03]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:03]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:05]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfeb5d270>, raw=Message(id='msg_bdrk_01JPoQZiYZLfa2fDkwW8DLif', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=45)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await claude_bedrock_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous streaming text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:05]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:05]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! I'm\n",
      "Hello! I'm just\n",
      "Hello! I'm just a\n",
      "Hello! I'm just a program\n",
      "Hello! I'm just a program,\n",
      "Hello! I'm just a program, so\n",
      "Hello! I'm just a program, so I\n",
      "Hello! I'm just a program, so I don't\n",
      "Hello! I'm just a program, so I don't have\n",
      "Hello! I'm just a program, so I don't have feelings\n",
      "Hello! I'm just a program, so I don't have feelings,\n",
      "Hello! I'm just a program, so I don't have feelings, but\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you.\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:06]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = openai_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chunk are the combination of the current and past chunks, and `stream_text` method is designed to return the result finally same as the response from the `generate_text` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfebb3160>, raw=<openai.Stream object at 0x7f7bff78f010>, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('root', Usage(model_name='gpt-4o-mini-2024-07-18', prompt_tokens=14, output_tokens=29))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.usage.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other client has the same interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:06]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:06]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! I'm\n",
      "Hello! I'm just\n",
      "Hello! I'm just a\n",
      "Hello! I'm just a computer\n",
      "Hello! I'm just a computer program\n",
      "Hello! I'm just a computer program,\n",
      "Hello! I'm just a computer program, so\n",
      "Hello! I'm just a computer program, so I\n",
      "Hello! I'm just a computer program, so I don't\n",
      "Hello! I'm just a computer program, so I don't have\n",
      "Hello! I'm just a computer program, so I don't have feelings\n",
      "Hello! I'm just a computer program, so I don't have feelings,\n",
      "Hello! I'm just a computer program, so I don't have feelings, but\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you.\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:08]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = azure_openai_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfebb2650>, raw=<openai.Stream object at 0x7f7bff0ff190>, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on Google AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:08]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:08]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you for asking! As a large language model,\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:09]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamed_response = google_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bff0fc5e0>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='m functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=None, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=None, parsed=None), name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:09]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:09]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "I'm doing well, thank you for asking! As a large language model,\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:11]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamed_response = vertexai_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfe24fd60>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='m functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=None, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=None, parsed=None), name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:11]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:11]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As\n",
      "Hello! As an AI language model, I don't have feelings\n",
      "Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist\n",
      "Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have.\n",
      "Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:12]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = claude_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfe26bfa0>, raw=None, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:12]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:12]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! As\n",
      "Hello! As an\n",
      "Hello! As an AI\n",
      "Hello! As an AI language\n",
      "Hello! As an AI language model\n",
      "Hello! As an AI language model,\n",
      "Hello! As an AI language model, I\n",
      "Hello! As an AI language model, I don\n",
      "Hello! As an AI language model, I don't\n",
      "Hello! As an AI language model, I don't have\n",
      "Hello! As an AI language model, I don't have feelings\n",
      "Hello! As an AI language model, I don't have feelings or\n",
      "Hello! As an AI language model, I don't have feelings or emotions\n",
      "Hello! As an AI language model, I don't have feelings or emotions,\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly an\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have.\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:14]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = claude_bedrock_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfebee950>, raw=None, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converse API model for Amazon Bedrock, here is the Amazon Nova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:14]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:14]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! I\n",
      "Hello! I'\n",
      "Hello! I'm\n",
      "Hello! I'm doing\n",
      "Hello! I'm doing well\n",
      "Hello! I'm doing well,\n",
      "Hello! I'm doing well, thank\n",
      "Hello! I'm doing well, thank you\n",
      "Hello! I'm doing well, thank you for\n",
      "Hello! I'm doing well, thank you for asking\n",
      "Hello! I'm doing well, thank you for asking.\n",
      "Hello! I'm doing well, thank you for asking. How\n",
      "Hello! I'm doing well, thank you for asking. How can\n",
      "Hello! I'm doing well, thank you for asking. How can I\n",
      "Hello! I'm doing well, thank you for asking. How can I assist\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today?\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic,\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic, feel\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic, feel free\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic, feel free to\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic, feel free to let\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic, feel free to let me\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic, feel free to let me know\n",
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic, feel free to let me know.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:15]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic, feel free to let me know.\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic, feel free to let me know.\n"
     ]
    }
   ],
   "source": [
    "streamed_response = bedrock_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm doing well, thank you for asking. How can I assist you today? If you have any questions or need information on a particular topic, feel free to let me know.\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfe24f1c0>, raw={'metadata': {'usage': {'inputTokens': 7, 'outputTokens': 40, 'totalTokens': 47}, 'metrics': {'latencyMs': 462}}}, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronously streaming text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stream_text_async` method is responsible for asynchronous streaming text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:15]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:15]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! I'm\n",
      "Hello! I'm just\n",
      "Hello! I'm just a\n",
      "Hello! I'm just a program\n",
      "Hello! I'm just a program,\n",
      "Hello! I'm just a program, so\n",
      "Hello! I'm just a program, so I\n",
      "Hello! I'm just a program, so I don't\n",
      "Hello! I'm just a program, so I don't have\n",
      "Hello! I'm just a program, so I don't have feelings\n",
      "Hello! I'm just a program, so I don't have feelings,\n",
      "Hello! I'm just a program, so I don't have feelings, but\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you.\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:16]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = openai_agent.stream_text_async(prompt=prompt1)\n",
    "\n",
    "async for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfe27fdf0>, raw=<openai.AsyncStream object at 0x7f7bfebeeb30>, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as other client, so skip demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In langrila, every client accepts `system_instruction` argument. This argument can be specified in both constructor and `generate_text(_async)`/`stream_text(_async)` method. The later case overrides constructor's system_instruction. System instruction needs to be `SystemPrompt` instance because OpenAI API needs role property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import SystemPrompt\n",
    "\n",
    "system_prompt = SystemPrompt(\n",
    "    role=\"system\",  # or developer role used for the models later than o1 for OpenAI API.\n",
    "    contents=\"You must to answer the question in Japanese.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:17]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:17]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:18]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='')], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfe24f940>, raw=ChatCompletion(id='chatcmpl-ApPZxkwlXDszbGtZ7CDGyZilJrT4U', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736816417, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_bd83329f63', usage=CompletionUsage(completion_tokens=15, prompt_tokens=27, total_tokens=42, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'system', 'content': 'You must to answer the question in Japanese.'}, {'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:18]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:18]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:19]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='')], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfe24dae0>, raw=ChatCompletion(id='chatcmpl-ApPZyltFuu8xxwzKlRaAleE52YOWU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1736816418, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_65792305e4', usage=CompletionUsage(completion_tokens=18, prompt_tokens=27, total_tokens=45, completion_tokens_details=None, prompt_tokens_details=None), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]), name=None, is_last_chunk=None, prompt=[{'role': 'system', 'content': 'You must to answer the question in Japanese.'}, {'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = azure_openai_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on Google AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:19]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:19]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:20]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='')], usage=<langrila.core.usage.NamedUsage object at 0x7f7bff0c4f10>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=7, prompt_token_count=18, total_token_count=25), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = google_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:20]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:20]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:22]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='')], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfe24ce20>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=7, prompt_token_count=16, total_token_count=23), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = vertexai_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:22]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:22]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:23]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='')], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfe13bc10>, raw=Message(id='msg_01BfLFMxpceAtgdp5s868Vmk', content=[TextBlock(text='', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=23, output_tokens=28, cache_creation_input_tokens=0, cache_read_input_tokens=0)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:23]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:23]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:25]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Assistant')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='Assistant')], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfe24ee90>, raw=Message(id='msg_bdrk_01RmRBktfVKD2NN7NYSvNbtH', content=[TextBlock(text='Assistant', type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=23, output_tokens=64)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_bedrock_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converse API for Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:25]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:25]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:26]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='')], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfe24ffa0>, raw={'ResponseMetadata': {'RequestId': 'df8c2eb3-446d-4f67-b854-d4c982a71b0f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 14 Jan 2025 01:00:26 GMT', 'content-type': 'application/json', 'content-length': '259', 'connection': 'keep-alive', 'x-amzn-requestid': 'df8c2eb3-446d-4f67-b854-d4c982a71b0f'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': ''}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 16, 'outputTokens': 15, 'totalTokens': 31}, 'metrics': {'latencyMs': 287}}, name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = bedrock_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response schema using tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response schema in langrila is just tool calling. Generated args are validated with pydantic, then is finally output as text response. This feature is supported by the `response_schema_as_tool` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Language(str, Enum):\n",
    "    japanese = \"Japanese\"\n",
    "    english = \"English\"\n",
    "    french = \"French\"\n",
    "    german = \"German\"\n",
    "\n",
    "\n",
    "class GreetingTime(str, Enum):\n",
    "    morning = \"morning\"\n",
    "    afternoon = \"afternoon\"\n",
    "    evening = \"evening\"\n",
    "\n",
    "\n",
    "class Greeting(BaseModel):\n",
    "    language: Language = Field(..., title=\"Language\", description=\"Language to greet in.\")\n",
    "    greeting: str = Field(..., title=\"Greeting\", description=\"Greeting message.\")\n",
    "    time: GreetingTime = Field(..., title=\"Time\", description=\"Time of the day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\n",
    "tell me the greeting message and the time of the day to use it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:26]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:26]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:28]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [ToolCallResponse(name='final_answer', args='{\"language\":\"Japanese\",\"greeting\":\"\",\"time\":\"morning\"}', call_id='call_LBMJEtW1lVL6XnwSHrPFkIBR')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:28]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:28]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:28]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"language\": \"Japanese\", \"greeting\": \"\", \"time\": \"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    response_schema_as_tool=Greeting,  # <- This is the arg to use the schema as a tool.\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.japanese: 'Japanese'>, greeting='', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`response_schema_as_tool` argument can be specified in generation method as well. If you pass this argument in both constructor and generation method, the later is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:28]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:28]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:29]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Let\\'s choose Japanese. \\n\\nIn Japanese, you can greet someone with \"\" (Konnichiwa) which means \"Hello.\" This greeting is typically used in the afternoon.')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:29]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text=\"Decide the next action based on the conversation. If you have all information for answering the user, run 'final_result' tool. If you need more information, invoke other tool to get necessary information. \")]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:29]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:30]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [ToolCallResponse(name='final_answer', args='{\"language\":\"Japanese\",\"greeting\":\"\",\"time\":\"afternoon\"}', call_id='call_U8JU87S21tvwvkORZ321Kun4')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:30]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:30]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:30]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"language\": \"Japanese\", \"greeting\": \"\", \"time\": \"afternoon\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\n",
    "tell me the greeting message and the time of the day to use it.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt, response_schema_as_tool=Greeting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.japanese: 'Japanese'>, greeting='', time=<GreetingTime.afternoon: 'afternoon'>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, unless you specify the response schema when instantiating the agent, response schema is reset at every execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:30]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:30]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:32]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Let\\'s choose **French**.\\n\\nA common greeting in French is **\"Bonjour.\"**\\n\\n- **Time of the Day to Use:** \"Bonjour\" is typically used in the morning until late afternoon, roughly from morning until around 6 PM. After this time, you would usually switch to \"Bonsoir,\" which means \"Good evening.\" \\n\\nSo, when you meet someone during the day, you can say \"Bonjour!\"')]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's choose **French**.\n",
      "\n",
      "A common greeting in French is **\"Bonjour.\"**\n",
      "\n",
      "- **Time of the Day to Use:** \"Bonjour\" is typically used in the morning until late afternoon, roughly from morning until around 6 PM. After this time, you would usually switch to \"Bonsoir,\" which means \"Good evening.\" \n",
      "\n",
      "So, when you meet someone during the day, you can say \"Bonjour!\"\n"
     ]
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=prompt)\n",
    "\n",
    "print(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response schema as a tool is named `final_answer`, and its default description is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final answer which ends this conversation. Arguments of this tool must be selected from the conversation history.\n",
      "Unkown argument in the entire conversation history must be null, however, the argument appeared in the previous conversation must be provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langrila import AgentConfig\n",
    "\n",
    "agent_config = AgentConfig()\n",
    "print(agent_config.final_answer_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter is configurable, so if you want to use another description, please change the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config = AgentConfig(\n",
    "    final_answer_description=\"This is example of final answer description.\",\n",
    ")\n",
    "\n",
    "_agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    response_schema_as_tool=Greeting,\n",
    "    agent_config=agent_config,  # Set the agent configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:32]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:32]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:34]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I can help with that! But, I need you to specify the language and time of day you'd like to use. Please choose one language from Japanese, English, French, and German, and one time of day from morning, afternoon, or evening.\")]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:34]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text=\"Decide the next action based on the conversation. If you have all information for answering the user, run 'final_result' tool. If you need more information, invoke other tool to get necessary information. \")]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:34]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:35]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I need more information to provide a greeting. I'll ask the user to specify the language and time of day they'd like to use.\")]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:35]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text=\"Decide the next action based on the conversation. If you have all information for answering the user, run 'final_result' tool. If you need more information, invoke other tool to get necessary information. \")]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:35]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:37]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [ToolCallResponse(name='final_answer', args='{\"greeting\": \"Good morning\", \"language\": \"English\", \"time\": \"morning\"}', call_id='4VSww6jRjxj6lm4HksqV0skn')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:37]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:37]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:37]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"greeting\": \"Good morning\", \"language\": \"English\", \"time\": \"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    response_schema_as_tool=Greeting,\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.english: 'English'>, greeting='Good morning', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    client=anthropic_client,\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=500,\n",
    "    response_schema_as_tool=Greeting,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:37]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:37]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:39]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Certainly! I'd be happy to provide you with a greeting in one of the languages you mentioned. I'll choose a language, provide a greeting message, and specify a time of day to use it. Let me prepare that information for you.\"), ToolCallResponse(name='final_answer', args='{\"language\": \"French\", \"greeting\": \"Bonjour!\", \"time\": \"morning\"}', call_id='toolu_01KqmyJLVodMNwSwZ35VcKrw')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:39]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:39]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:39]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"language\": \"French\", \"greeting\": \"Bonjour!\", \"time\": \"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.french: 'French'>, greeting='Bonjour!', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converse API for Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:39]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:39]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:40]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='<thinking>The User has asked for a greeting in one of the four specified languages. I will randomly select one language from the list and provide the corresponding greeting message and the appropriate time of the day to use it.</thinking>\\n'), ToolCallResponse(name='final_answer', args='{\"greeting\": \"\", \"language\": \"Japanese\", \"time\": \"morning\"}', call_id='tooluse_7tHknRD7SkCNLpOJDNRulQ')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:40]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:40]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:40]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"greeting\": \"\", \"language\": \"Japanese\", \"time\": \"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=bedrock_client,\n",
    "    modelId=\"us.amazon.nova-lite-v1:0\",\n",
    "    response_schema_as_tool=Greeting,\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.japanese: 'Japanese'>, greeting='', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response schema natively supported by each provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except a few arguments, Agent module accepts almost all parameters natively supported by each provider as it is. For example, if you want to use native response schema for OpenAI API or Gemini, you can specify that parameter supported by each API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:40]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:40]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:41]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='{\"language\":\"Japanese\",\"greeting\":\"\",\"time\":\"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\n",
    "tell me the greeting message and the time of the day to use it.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    response_format=Greeting,  # native parameter for structured output\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.japanese: 'Japanese'>, greeting='', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:42]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:42]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:44]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='{\\n  \"greeting\": \"Konnichiwa\",\\n  \"language\": \"Japanese\",\\n  \"time\": \"afternoon\"\\n}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Gemini can't support respones schema includes $defs and $ref so we need to flatten the schema.\n",
    "# Also parameter type must be upper case.\n",
    "\n",
    "from langrila.google.gemini_utils import to_gemini_schema\n",
    "\n",
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=to_gemini_schema(Greeting),  # native parameter for structured output\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.japanese: 'Japanese'>, greeting='Konnichiwa', time=<GreetingTime.afternoon: 'afternoon'>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that different provider has the different specification and limitation of the native response schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize internal prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some agent framework uses internal prompt that is not customizable. This might affect the agent behaviour using another language like Japanese. Agent in langrila has internal prompts as well, for example, that is used for validation error retry, but that is customizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import InternalPrompt\n",
    "\n",
    "internal_prompt = InternalPrompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the internal prompts are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error_retry': 'Please fix the error on tool use. If the validation error is raised, reflect the conversation history and try to find the correct answer. If there is no fact to answer the user, try to run other tools to get necessary information. ',\n",
       " 'no_tool_use_retry': \"Decide the next action based on the conversation. If you have all information for answering the user, run 'final_result' tool. If you need more information, invoke other tool to get necessary information. \",\n",
       " 'planning': 'Please make a concise plan to answer the following question/requirement, considering the conversation history.\\nYou can invoke the sub-agents or tools to answer the questions/requirements shown in the capabilities section.\\nAgent has no description while the tools have a description.\\n\\nQuestion/Requirement:\\n{user_input}\\n\\nCapabilities:\\n{capabilities}',\n",
       " 'do_plan': 'Put the plan into action.'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_prompt.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to customize internal prompt, you can configure it via `AgentConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import AgentConfig\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    internal_prompt=InternalPrompt(\n",
    "        error_retry=\"\",  # replace retry prompt with Japanese\n",
    "    ),\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    agent_config=agent_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langrila supports o1-family for OpenAI API. (Precisely, langrila doesn't depend on the model name.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"o1-mini-2024-09-12\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:45]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-14 10:00:45]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-14 10:00:48]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm doing well, thank you for asking. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm doing well, thank you for asking. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f7bfe24fc70>, raw=ChatCompletion(id='chatcmpl-ApPaPeNI8z8EtCSh5zWpY8IGdmOFW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm doing well, thank you for asking. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736816445, model='o1-mini-2024-09-12', object='chat.completion', service_tier='default', system_fingerprint='fp_f56e40de61', usage=CompletionUsage(completion_tokens=350, prompt_tokens=14, total_tokens=364, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=320, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('root', Usage(model_name='o1-mini-2024-09-12', prompt_tokens=14, output_tokens=350))])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
