{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"NONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure your environmental variables and dependencies are ready to use LLM services. Name of the environmental variables is arbitraray because langrila modules accepts that name as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env_api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main modules are `{Provider}Client` module and `Agent` module. `{Provider}Client` module is inherited by each module for a specific llm provider like OpenAI, Google or Anthropic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import Agent\n",
    "from langrila.anthropic import AnthropicClient\n",
    "from langrila.google import GoogleClient\n",
    "from langrila.openai import OpenAIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating client modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenAI\n",
    "openai_client = OpenAIClient(api_key_env_name=\"OPENAI_API_KEY\")\n",
    "\n",
    "# For Azure OpenAI\n",
    "azure_openai_client = OpenAIClient(\n",
    "    api_key_env_name=\"AZURE_API_KEY\",\n",
    "    api_type=\"azure\",\n",
    "    azure_api_version=\"2024-11-01-preview\",\n",
    "    azure_endpoint_env_name=\"AZURE_ENDPOINT\",\n",
    "    azure_deployment_id_env_name=\"AZURE_DEPLOYMENT_ID\",\n",
    ")\n",
    "\n",
    "# For Gemini on Google AI Studio\n",
    "google_dev_client = GoogleClient(api_key_env_name=\"GEMINI_API_KEY\")\n",
    "\n",
    "# For Gemini on Google Cloud VertexAI\n",
    "vertexai_client = GoogleClient(\n",
    "    api_type=\"vertexai\",\n",
    "    project_id_env_name=\"GOOGLE_CLOUD_PROJECT\",\n",
    "    location=\"us-central1\",\n",
    ")\n",
    "\n",
    "# For Claude of Anthropic\n",
    "anthropic_client = AnthropicClient(api_key_env_name=\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# For Claude of Amazon Bedrock\n",
    "claude_bedrock_client = AnthropicClient(\n",
    "    api_type=\"bedrock\",\n",
    "    aws_access_key_env_name=\"AWS_ACCESS_KEY\",\n",
    "    aws_secret_key_env_name=\"AWS_SECRET_KEY\",\n",
    "    aws_region_env_name=\"AWS_REGION\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass any client instance to `Agent`. Almost all arguments is the same as the provider's sdk. Arguments of the `Agent` class are finally passed to the `{Provider}Client` module almost as it is, and `{Provider}Client` is just simple wrapper module of each LLM provider API. So please look at the API reference of each provider API if you want to know what arguments can be accepted by `Agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "azure_openai_agent = Agent(\n",
    "    client=azure_openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "google_agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "vertexai_agent = Agent(\n",
    "    client=vertexai_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "claude_agent = Agent(\n",
    "    client=anthropic_client,\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "claude_bedrock_agent = Agent(\n",
    "    client=claude_bedrock_client,\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can input your prompt in 2 ways; input string prompt directly or create `Prompt` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import Prompt, TextPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple string prompt\n",
    "prompt1 = \"Hello. How are you today?\"\n",
    "\n",
    "# 2. Prompt instance with raw string\n",
    "prompt2 = Prompt(\n",
    "    role=\"user\",\n",
    "    contents=\"Hello. How are you today?\",\n",
    ")\n",
    "\n",
    "# 3. Prompt instance with list of TextPrompt\n",
    "prompt3 = Prompt(\n",
    "    role=\"user\",\n",
    "    contents=[\n",
    "        TextPrompt(\n",
    "            text=\"Hello. How are you today?\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pattern 2, string specified as `contents` argument is automatically converted to list of `TextPrompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2 == prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(type='Prompt', role='user', contents=[TextPrompt(text='Hello. How are you today?')], name=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(type='Prompt', role='user', contents=[TextPrompt(text='Hello. How are you today?')], name=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous, non-streaming text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:41]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:41]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:42]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = openai_agent.generate_text(prompt=prompt1)\n",
    "\n",
    "# Same as\n",
    "# response = openai_agent.generate_text(prompt=prompt2)\n",
    "# response = openai_agent.generate_text(prompt=prompt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response object is a pydantic model contains formatted response from the provider API, usage, and raw response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fde0435f970>, raw=ChatCompletion(id='chatcmpl-AnnzCtVoxMGxOO9Tz6sKgcQX807h5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736433582, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f2cd28694a', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent class accumulates usage across all API call and interaction with subagent. Sub-agent has its own name, and root agent is named \"root\". You can access this accumulated usage via `response.usage`. `response.usage` is dict-like object, and it has `items()` method and `__getitem__()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('root', Usage(model_name='gpt-4o-mini-2024-07-18', prompt_tokens=14, output_tokens=30))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(model_name='gpt-4o-mini-2024-07-18', prompt_tokens=14, output_tokens=30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage[\"root\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access raw response from the API that is helpful fo9r debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AnnzCtVoxMGxOO9Tz6sKgcQX807h5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736433582, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f2cd28694a', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as other client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:42]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:42]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:44]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddd0b4afe0>, raw=ChatCompletion(id='chatcmpl-AnnzECbhlflRLvtqHbFxTJAr2qyC0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1736433584, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_5154047bf2', usage=CompletionUsage(completion_tokens=29, prompt_tokens=14, total_tokens=43, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = azure_openai_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AnnzECbhlflRLvtqHbFxTJAr2qyC0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1736433584, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_5154047bf2', usage=CompletionUsage(completion_tokens=29, prompt_tokens=14, total_tokens=43, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on Google AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:44]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:44]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:45]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddd031bf10>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = google_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:46]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:46]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:48]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddd0319960>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = vertexai_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude of Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:48]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:48]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:49]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddd0b4bb50>, raw=Message(id='msg_01VqzgRRfftfsg3i55ZMoVb7', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=43, cache_creation_input_tokens=0, cache_read_input_tokens=0)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01VqzgRRfftfsg3i55ZMoVb7', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=43, cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:49]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:49]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:51]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddd031b9a0>, raw=Message(id='msg_bdrk_01KnPF2BTRkRPzcQwHMMBRx8', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=45)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_bedrock_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_bdrk_01KnPF2BTRkRPzcQwHMMBRx8', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=45))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor params vs generation params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except a few parameters, almost all the parameters can be specified in both constructor and generation method (such as `generate_text()` method). Constructor params in `Agent` class can be overriden by the arguments in `generate_text` method (as well as `stream_text(_async)` method as shown later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:51]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello!')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:51]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:52]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Hello there! How can I help you today?')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='Hello there! How can I help you today?')], usage=<langrila.core.usage.NamedUsage object at 0x7fddcff5cd00>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello there! How can I help you today?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=11, prompt_token_count=3, total_token_count=14), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    temperature=0.0,\n",
    "    max_output_tokens=500,\n",
    ")\n",
    "\n",
    "agent.generate_text(prompt=\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is the same as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:52]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello!')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:52]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:54]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Hello there! How can I help you today?')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='Hello there! How can I help you today?')], usage=<langrila.core.usage.NamedUsage object at 0x7fddd0b4b820>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello there! How can I help you today?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=11, prompt_token_count=3, total_token_count=14), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    ")\n",
    "\n",
    "agent.generate_text(\n",
    "    prompt=\"Hello!\", model=\"gemini-2.0-flash-exp\", temperature=0.0, max_output_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing arguments when initiating is useful to reuse same parameters at multiple place. Especiall, multi-agent system invokes multiple agents at the same time, so it requires many parameters to be static. On the other hand, passing arguments when generating helps you control settings whenever you call the API. This is helpful for the single agent case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, here is an example to specify the same parameter when initializing agent and generating response. (As I mentioned, the later parameter is prioritized.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    n=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:54]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello! How are you?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:54]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:55]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a computer program, but I'm here and ready to help you. How can I assist you today?\"), TextResponse(text=\"Hello! I'm just a computer program, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=\"Hello! How are you?\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextResponse(text=\"Hello! I'm just a computer program, but I'm here and ready to help you. How can I assist you today?\"),\n",
       " TextResponse(text=\"Hello! I'm just a computer program, but I'm here and ready to help you. How can I assist you today?\")]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `n` parameter is specified in both constructor and `generate_text` method, but `n=2` is used, which was specified for `generate_text` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous, non-streaming text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate_text_async` method allows you to received text asyncronously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:55]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:55]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:56]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcff4fdc0>, raw=ChatCompletion(id='chatcmpl-AnnzQXiQPjI0BfNhkl9ITiU45yR6l', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736433596, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f2cd28694a', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await openai_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:56]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:56]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:57]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcff76c50>, raw=ChatCompletion(id='chatcmpl-AnnzRpgyw3GV7WoujspcULhoRGEnv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1736433597, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_f3927aa00d', usage=CompletionUsage(completion_tokens=29, prompt_tokens=14, total_tokens=43, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await azure_openai_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:57]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:57]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:59]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddd03f17b0>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await google_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:39:59]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:39:59]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:01]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcff4f820>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await vertexai_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude of Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:01]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:01]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:02]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcff5e470>, raw=Message(id='msg_01RcJHMaaggSrE5Xm9htamEE', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=43, cache_creation_input_tokens=0, cache_read_input_tokens=0)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await claude_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude of Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:02]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:02]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:03]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcffdb550>, raw=Message(id='msg_bdrk_01DgyEk9dEEaFfG8v3HYuy9v', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=45)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await claude_bedrock_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous streaming text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:04]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:04]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! I'm\n",
      "Hello! I'm just\n",
      "Hello! I'm just a\n",
      "Hello! I'm just a program\n",
      "Hello! I'm just a program,\n",
      "Hello! I'm just a program, so\n",
      "Hello! I'm just a program, so I\n",
      "Hello! I'm just a program, so I don't\n",
      "Hello! I'm just a program, so I don't have\n",
      "Hello! I'm just a program, so I don't have feelings\n",
      "Hello! I'm just a program, so I don't have feelings,\n",
      "Hello! I'm just a program, so I don't have feelings, but\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you.\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:04]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = openai_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chunk are the combination of the current and past chunks, and `stream_text` method is designed to return the result finally same as the response from the `generate_text` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcffda980>, raw=<openai.Stream object at 0x7fddcff5dea0>, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('root', Usage(model_name='gpt-4o-mini-2024-07-18', prompt_tokens=14, output_tokens=29))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.usage.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other client has the same interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:04]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:04]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! I'm\n",
      "Hello! I'm just\n",
      "Hello! I'm just a\n",
      "Hello! I'm just a program\n",
      "Hello! I'm just a program,\n",
      "Hello! I'm just a program, so\n",
      "Hello! I'm just a program, so I\n",
      "Hello! I'm just a program, so I don't\n",
      "Hello! I'm just a program, so I don't have\n",
      "Hello! I'm just a program, so I don't have feelings\n",
      "Hello! I'm just a program, so I don't have feelings,\n",
      "Hello! I'm just a program, so I don't have feelings, but\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you.\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:08]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = azure_openai_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcff5e3b0>, raw=<openai.Stream object at 0x7fddd031b940>, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on Google AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:08]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:08]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "I'm doing well, thank you for asking! As a large language model,\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamed_response = google_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcffefc70>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='m functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=None, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=None, parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:09]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:09]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "I'm doing well, thank you for asking! As a large language model,\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamed_response = vertexai_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcffefeb0>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='m functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=None, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=None, parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:11]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:11]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As\n",
      "Hello! As an AI language model, I don't have feelings\n",
      "Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or\n",
      "Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:13]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = claude_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcffef7c0>, raw=None, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:13]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:13]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! As\n",
      "Hello! As an\n",
      "Hello! As an AI\n",
      "Hello! As an AI language\n",
      "Hello! As an AI language model\n",
      "Hello! As an AI language model,\n",
      "Hello! As an AI language model, I\n",
      "Hello! As an AI language model, I don\n",
      "Hello! As an AI language model, I don't\n",
      "Hello! As an AI language model, I don't have\n",
      "Hello! As an AI language model, I don't have feelings\n",
      "Hello! As an AI language model, I don't have feelings or\n",
      "Hello! As an AI language model, I don't have feelings or emotions\n",
      "Hello! As an AI language model, I don't have feelings or emotions,\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly an\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have.\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:16]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = claude_bedrock_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcff4f7c0>, raw=None, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronously streaming text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stream_text_async` method is responsible for asynchronous streaming text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:16]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:16]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! I'm\n",
      "Hello! I'm just\n",
      "Hello! I'm just a\n",
      "Hello! I'm just a program\n",
      "Hello! I'm just a program,\n",
      "Hello! I'm just a program, so\n",
      "Hello! I'm just a program, so I\n",
      "Hello! I'm just a program, so I don't\n",
      "Hello! I'm just a program, so I don't have\n",
      "Hello! I'm just a program, so I don't have feelings\n",
      "Hello! I'm just a program, so I don't have feelings,\n",
      "Hello! I'm just a program, so I don't have feelings, but\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you.\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:17]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = openai_agent.stream_text_async(prompt=prompt1)\n",
    "\n",
    "async for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcffef940>, raw=<openai.AsyncStream object at 0x7fddcff76860>, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as other client, so skip demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In langrila, every client accepts `system_instruction` argument. This argument can be specified in both constructor and `generate_text(_async)`/`stream_text(_async)` method. The later case overrides constructor's system_instruction. System instruction needs to be `SystemPrompt` instance because OpenAI API needs role property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import SystemPrompt\n",
    "\n",
    "system_prompt = SystemPrompt(\n",
    "    role=\"system\",  # or developer role used for the models later than o1 for OpenAI API.\n",
    "    contents=\"You must to answer the question in Japanese.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:17]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:17]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:18]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='こんにちは。今日は元気です。あなたはいかがですか？')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='こんにちは。今日は元気です。あなたはいかがですか？')], usage=<langrila.core.usage.NamedUsage object at 0x7fddcffda230>, raw=ChatCompletion(id='chatcmpl-AnnzmmaMQQ9jiaaQEnMmqOL9TbuYU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='こんにちは。今日は元気です。あなたはいかがですか？', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736433618, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0aa8d3e20b', usage=CompletionUsage(completion_tokens=15, prompt_tokens=27, total_tokens=42, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'system', 'content': 'You must to answer the question in Japanese.'}, {'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:18]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:18]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:19]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='こんにちは。今日は元気です。あなたはいかがですか？')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='こんにちは。今日は元気です。あなたはいかがですか？')], usage=<langrila.core.usage.NamedUsage object at 0x7fddcffdb2b0>, raw=ChatCompletion(id='chatcmpl-Annzo2wJSRTzar4G1wlOyE84qboJL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='こんにちは。今日は元気です。あなたはいかがですか？', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1736433620, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_5154047bf2', usage=CompletionUsage(completion_tokens=14, prompt_tokens=27, total_tokens=41, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]), name=None, is_last_chunk=None, prompt=[{'role': 'system', 'content': 'You must to answer the question in Japanese.'}, {'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = azure_openai_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on Google AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:19]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:19]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:21]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='こんにちは。今日は元気ですか？')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='こんにちは。今日は元気ですか？')], usage=<langrila.core.usage.NamedUsage object at 0x7fddcff4fe20>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='こんにちは。今日は元気ですか？\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=7, prompt_token_count=18, total_token_count=25), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = google_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:21]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:21]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:22]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='こんにちは。今日は元気ですか？')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='こんにちは。今日は元気ですか？')], usage=<langrila.core.usage.NamedUsage object at 0x7fddd031bbb0>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='こんにちは。今日は元気ですか？\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=7, prompt_token_count=16, total_token_count=23), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = vertexai_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:22]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:22]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:23]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='こんにちは。お元気ですか？私は人工知能のアシスタントですが、日本語でお答えするようにとの指示を受けましたので、日本語でお返事いたします。今日のお天気はいかがですか？')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='こんにちは。お元気ですか？私は人工知能のアシスタントですが、日本語でお答えするようにとの指示を受けましたので、日本語でお返事いたします。今日のお天気はいかがですか？')], usage=<langrila.core.usage.NamedUsage object at 0x7fddcffef7f0>, raw=Message(id='msg_01CmgNUUotkRqz7yCESWRVfy', content=[TextBlock(text='こんにちは。お元気ですか？私は人工知能のアシスタントですが、日本語でお答えするようにとの指示を受けましたので、日本語でお返事いたします。今日のお天気はいかがですか？', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=23, output_tokens=76, cache_creation_input_tokens=0, cache_read_input_tokens=0)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:23]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:23]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:27]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='こんにちは。私は人工知能のAssistantですので、気分というものはありませんが、きちんと動作しています。何かお手伝いできることがあれば、喜んでお答えします。')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='こんにちは。私は人工知能のAssistantですので、気分というものはありませんが、きちんと動作しています。何かお手伝いできることがあれば、喜んでお答えします。')], usage=<langrila.core.usage.NamedUsage object at 0x7fddcf6b69b0>, raw=Message(id='msg_bdrk_01WcgsM5ewBfGyRgYBFHFZsH', content=[TextBlock(text='こんにちは。私は人工知能のAssistantですので、気分というものはありませんが、きちんと動作しています。何かお手伝いできることがあれば、喜んでお答えします。', type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=23, output_tokens=64)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_bedrock_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response schema using tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response schema in langrila is just tool calling. Generated args are validated with pydantic, then is finally output as text response. This feature is supported by the `response_schema_as_tool` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Language(str, Enum):\n",
    "    japanese = \"Japanese\"\n",
    "    english = \"English\"\n",
    "    french = \"French\"\n",
    "    german = \"German\"\n",
    "\n",
    "\n",
    "class GreetingTime(str, Enum):\n",
    "    morning = \"morning\"\n",
    "    afternoon = \"afternoon\"\n",
    "    evening = \"evening\"\n",
    "\n",
    "\n",
    "class Greeting(BaseModel):\n",
    "    language: Language = Field(..., title=\"Language\", description=\"Language to greet in.\")\n",
    "    greeting: str = Field(..., title=\"Greeting\", description=\"Greeting message.\")\n",
    "    time: GreetingTime = Field(..., title=\"Time\", description=\"Time of the day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\n",
    "tell me the greeting message and the time of the day to use it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:27]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:27]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [ToolCallResponse(name='final_answer', args='{\"language\": \"Japanese\", \"greeting\": \"おはようございます\", \"time\": \"morning\"}', call_id='call_6qWUSr2itA9NahF3czfW8sFq'), ToolCallResponse(name='final_answer', args='{\"language\": \"English\", \"greeting\": \"Good Afternoon\", \"time\": \"afternoon\"}', call_id='call_CNIBl0SbKJwGAI4lyoBUynvE'), ToolCallResponse(name='final_answer', args='{\"language\": \"French\", \"greeting\": \"Bonsoir\", \"time\": \"evening\"}', call_id='call_SHleO1EkLc9u11qBoFJbZSkF'), ToolCallResponse(name='final_answer', args='{\"language\": \"German\", \"greeting\": \"Guten Morgen\", \"time\": \"morning\"}', call_id='call_FolgQPkj5TsZJbLCsTCUIPti')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"language\": \"German\", \"greeting\": \"Guten Morgen\", \"time\": \"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    response_schema_as_tool=Greeting,  # <- This is the arg to use the schema as a tool.\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.german: 'German'>, greeting='Guten Morgen', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`response_schema_as_tool` argument can be specified in generation method as well. If you pass this argument in both constructor and generation method, the later is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:29]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:30]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Let\\'s choose Japanese. A common greeting in Japanese is \"おはようございます\" (Ohayō gozaimasu), which means \"Good morning.\" You would use this greeting in the morning.')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:30]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text=\"Decide the next action based on the conversation. If you have all information for answering the user, run 'final_result' tool. If you need more information, invoke other tool to get necessary information. \")]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:30]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:32]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [ToolCallResponse(name='final_answer', args='{\"language\":\"Japanese\",\"greeting\":\"おはようございます\",\"time\":\"morning\"}', call_id='call_HWgUhDLAFNlHxoHzWYBgNiMm')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:32]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:32]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:32]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"language\": \"Japanese\", \"greeting\": \"おはようございます\", \"time\": \"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\n",
    "tell me the greeting message and the time of the day to use it.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt, response_schema_as_tool=Greeting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.japanese: 'Japanese'>, greeting='おはようございます', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you specify the response schema when instantiating the agent, response schema is reset at every execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:32]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:32]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:33]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Let\\'s choose **French**. \\n\\nA common greeting is **\"Bonjour.\"** \\n\\nYou would use \"Bonjour\" during the daytime, typically until the early evening (around 5 PM). It means \"Good morning\" or \"Good day.\" \\n\\nIn the evening, you can switch to **\"Bonsoir,\"** which means \"Good evening.\"')]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's choose **French**. \n",
      "\n",
      "A common greeting is **\"Bonjour.\"** \n",
      "\n",
      "You would use \"Bonjour\" during the daytime, typically until the early evening (around 5 PM). It means \"Good morning\" or \"Good day.\" \n",
      "\n",
      "In the evening, you can switch to **\"Bonsoir,\"** which means \"Good evening.\"\n"
     ]
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=prompt)\n",
    "\n",
    "print(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response schema as a tool is named `final_answer`, and its default description is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final answer which ends this conversation. Arguments of this tool must be selected from the conversation history.\n",
      "Unkown argument in the entire conversation history must be null, however, the argument appeared in the previous conversation must be provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langrila import AgentConfig\n",
    "\n",
    "agent_config = AgentConfig()\n",
    "print(agent_config.final_answer_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter is configurable, so if you want to use another description, please change the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config = AgentConfig(\n",
    "    final_answer_description=\"This is example of final answer description.\",\n",
    ")\n",
    "\n",
    "_agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    response_schema_as_tool=Greeting,\n",
    "    agent_config=agent_config,  # Set the agent configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:34]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:34]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:35]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I can help with that! Please specify which language you'd like to use.\")]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:35]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text=\"Decide the next action based on the conversation. If you have all information for answering the user, run 'final_result' tool. If you need more information, invoke other tool to get necessary information. \")]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:35]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:36]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [ToolCallResponse(name='final_answer', args='{\"time\": \"morning\", \"greeting\": \"Good morning\", \"language\": \"English\"}', call_id='c3FME0L7TUGxrxqpN78g8h38')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:36]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:36]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:36]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"time\": \"morning\", \"greeting\": \"Good morning\", \"language\": \"English\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    response_schema_as_tool=Greeting,\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.english: 'English'>, greeting='Good morning', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    client=anthropic_client,\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=500,\n",
    "    response_schema_as_tool=Greeting,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:36]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:36]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:39]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Certainly! I'd be happy to provide you with a greeting in one of the languages you mentioned. Let me choose a language and provide you with a greeting along with the appropriate time of day to use it.\\n\\nI'll select French for this example.\"), ToolCallResponse(name='final_answer', args='{\"language\": \"French\", \"greeting\": \"Bonjour!\", \"time\": \"morning\"}', call_id='toolu_0176BXaJRXQZg3WnumE3nAph')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:39]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:39]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:39]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"language\": \"French\", \"greeting\": \"Bonjour!\", \"time\": \"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.french: 'French'>, greeting='Bonjour!', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response schema natively supported by each provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except a few arguments, Agent module accepts almost all parameters natively supported by each provider as it is. For example, if you want to use native response schema for OpenAI API or Gemini, you can specify that parameter supported by each API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:39]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:39]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:40]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='{\"language\":\"Japanese\",\"greeting\":\"おはようございます (Ohayou gozaimasu)\",\"time\":\"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\n",
    "tell me the greeting message and the time of the day to use it.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    response_format=Greeting,  # native parameter for structured output\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.japanese: 'Japanese'>, greeting='おはようございます (Ohayou gozaimasu)', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:40]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:40]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:42]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='{\\n  \"greeting\": \"こんにちは\",\\n  \"language\": \"Japanese\",\\n  \"time\": \"afternoon\"\\n}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Gemini can't support respones schema includes $defs and $ref so we need to flatten the schema.\n",
    "# Also parameter type must be upper case.\n",
    "\n",
    "from langrila.google.gemini_utils import to_gemini_schema\n",
    "\n",
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=to_gemini_schema(Greeting),  # native parameter for structured output\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.japanese: 'Japanese'>, greeting='こんにちは', time=<GreetingTime.afternoon: 'afternoon'>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that different provider has the different specification and limitation of the native response schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize internal prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some agent framework uses internal prompt that is not customizable. This might affect the agent behaviour using another language like Japanese. Agent in langrila has internal prompts as well, for example, that is used for validation error retry, but that is customizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import InternalPrompt\n",
    "\n",
    "internal_prompt = InternalPrompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the internal prompts are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error_retry': 'Please fix the error on tool use. If the validation error is raised, reflect the conversation history and try to find the correct answer. If there is no fact to answer the user, try to run other tools to get necessary information. ',\n",
       " 'no_tool_use_retry': \"Decide the next action based on the conversation. If you have all information for answering the user, run 'final_result' tool. If you need more information, invoke other tool to get necessary information. \",\n",
       " 'planning': 'Please make a concise plan to answer the following question/requirement, considering the conversation history.\\nYou can invoke the sub-agents or tools to answer the questions/requirements shown in the capabilities section.\\nAgent has no description while the tools have a description.\\n\\nQuestion/Requirement:\\n{user_input}\\n\\nCapabilities:\\n{capabilities}',\n",
       " 'do_plan': 'Put the plan into action.'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_prompt.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to customize internal prompt, you can configure it via `AgentConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import AgentConfig\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    internal_prompt=InternalPrompt(\n",
    "        error_retry=\"エラーを修正してください。\",  # replace retry prompt with Japanese\n",
    "    ),\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    agent_config=agent_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langrila supports o1-family for OpenAI API. (Precisely, langrila doesn't depend on the model name.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"o1-mini-2024-09-12\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-09 23:40:42]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:42]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-09 23:40:44]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm doing well, thank you for asking. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm doing well, thank you for asking. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7fddcf612bf0>, raw=ChatCompletion(id='chatcmpl-Ano0Be0NHMDWUR4qNRtmgkmNVT31S', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm doing well, thank you for asking. How can I help you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736433643, model='o1-mini-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_f56e40de61', usage=CompletionUsage(completion_tokens=158, prompt_tokens=14, total_tokens=172, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=128, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('root', Usage(model_name='o1-mini-2024-09-12', prompt_tokens=14, output_tokens=158))])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
