{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"NONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure your environmental variables and dependencies are ready to use LLM services. Name of the environmental variables is arbitraray because langrila modules accepts that name as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env_api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main modules are `{Provider}Client` module and `Agent` module. `{Provider}Client` module is inherited by each module for a specific llm provider like OpenAI, Google or Anthropic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import Agent\n",
    "from langrila.anthropic import AnthropicClient\n",
    "from langrila.google import GoogleClient\n",
    "from langrila.openai import OpenAIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating client modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenAI\n",
    "openai_client = OpenAIClient(api_key_env_name=\"OPENAI_API_KEY\")\n",
    "\n",
    "# For Azure OpenAI\n",
    "azure_openai_client = OpenAIClient(\n",
    "    api_key_env_name=\"AZURE_API_KEY\",\n",
    "    api_type=\"azure\",\n",
    "    azure_api_version=\"2024-11-01-preview\",\n",
    "    azure_endpoint_env_name=\"AZURE_ENDPOINT\",\n",
    "    azure_deployment_id_env_name=\"AZURE_DEPLOYMENT_ID\",\n",
    ")\n",
    "\n",
    "# For Gemini on Google AI Studio\n",
    "google_dev_client = GoogleClient(api_key_env_name=\"GEMINI_API_KEY\")\n",
    "\n",
    "# For Gemini on Google Cloud VertexAI\n",
    "vertexai_client = GoogleClient(\n",
    "    api_type=\"vertexai\",\n",
    "    project_id_env_name=\"GOOGLE_CLOUD_PROJECT\",\n",
    "    location=\"us-central1\",\n",
    ")\n",
    "\n",
    "# For Claude of Anthropic\n",
    "anthropic_client = AnthropicClient(api_key_env_name=\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# For Claude of Amazon Bedrock\n",
    "claude_bedrock_client = AnthropicClient(\n",
    "    api_type=\"bedrock\",\n",
    "    aws_access_key_env_name=\"AWS_ACCESS_KEY\",\n",
    "    aws_secret_key_env_name=\"AWS_SECRET_KEY\",\n",
    "    aws_region_env_name=\"AWS_REGION\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass any client instance to `Agent`. Almost all arguments is the same as the provider's sdk. Arguments of the `Agent` class are finally passed to the `{Provider}Client` module almost as it is, and `{Provider}Client` is just simple wrapper module of each LLM provider API. So please look at the API reference of each provider API if you want to know what arguments can be accepted by `Agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "azure_openai_agent = Agent(\n",
    "    client=azure_openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "google_agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "vertexai_agent = Agent(\n",
    "    client=vertexai_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "claude_agent = Agent(\n",
    "    client=anthropic_client,\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "claude_bedrock_agent = Agent(\n",
    "    client=claude_bedrock_client,\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can input your prompt in 2 ways; input string prompt directly or create `Prompt` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import Prompt, TextPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple string prompt\n",
    "prompt1 = \"Hello. How are you today?\"\n",
    "\n",
    "# 2. Prompt instance with raw string\n",
    "prompt2 = Prompt(\n",
    "    role=\"user\",\n",
    "    contents=\"Hello. How are you today?\",\n",
    ")\n",
    "\n",
    "# 3. Prompt instance with list of TextPrompt\n",
    "prompt3 = Prompt(\n",
    "    role=\"user\",\n",
    "    contents=[\n",
    "        TextPrompt(\n",
    "            text=\"Hello. How are you today?\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pattern 2, string specified as `contents` argument is automatically converted to list of `TextPrompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2 == prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(type='Prompt', role='user', contents=[TextPrompt(text='Hello. How are you today?')], name=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(type='Prompt', role='user', contents=[TextPrompt(text='Hello. How are you today?')], name=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous, non-streaming text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:49:31]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:49:31]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:49:32]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = openai_agent.generate_text(prompt=prompt1)\n",
    "\n",
    "# Same as\n",
    "# response = openai_agent.generate_text(prompt=prompt2)\n",
    "# response = openai_agent.generate_text(prompt=prompt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response object is a pydantic model contains formatted response from the provider API, usage, and raw response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed95cfb80>, raw=ChatCompletion(id='chatcmpl-AlC7Xz1fuiqjEwYguDprgrEvxTQ8S', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735811371, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_d02d531b47', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent class accumulates usage across all API call and interaction with subagent. Sub-agent has its own name, and root agent is named \"root\". You can access this accumulated usage via `response.usage`. `response.usage` is dict-like object, and it has `items()` method and `__getitem__()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('root', Usage(model_name='gpt-4o-mini-2024-07-18', prompt_tokens=14, output_tokens=30))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(model_name='gpt-4o-mini-2024-07-18', prompt_tokens=14, output_tokens=30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage[\"root\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access raw response from the API that is helpful fo9r debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AlC7Xz1fuiqjEwYguDprgrEvxTQ8S', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735811371, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_d02d531b47', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as other client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:49:38]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:49:38]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:49:40]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed8f85ea0>, raw=ChatCompletion(id='chatcmpl-AlC7eJNXpc66nKOQ1Og1a2AJEIcRQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_result={'error': {'code': 'content_filter_error', 'message': 'The contents are not filtered'}}, content_filter_results={})], created=1735811378, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_5154047bf2', usage=CompletionUsage(completion_tokens=29, prompt_tokens=14, total_tokens=43, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}]), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = azure_openai_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AlC7eJNXpc66nKOQ1Og1a2AJEIcRQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_result={'error': {'code': 'content_filter_error', 'message': 'The contents are not filtered'}}, content_filter_results={})], created=1735811378, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_5154047bf2', usage=CompletionUsage(completion_tokens=29, prompt_tokens=14, total_tokens=43, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on Google AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:50:15]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:50:15]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:50:16]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed97efbb0>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=CitationMetadata(citations=[Citation(end_index=139, license=None, publication_date=None, start_index=4, title=None, uri=None)]), finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = google_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=CitationMetadata(citations=[Citation(end_index=139, license=None, publication_date=None, start_index=4, title=None, uri=None)]), finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:50:33]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:50:33]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:50:34]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8f04f1b940>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=CitationMetadata(citations=[Citation(end_index=139, license=None, publication_date=None, start_index=4, title=None, uri=None)]), finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = vertexai_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=CitationMetadata(citations=[Citation(end_index=139, license=None, publication_date=None, start_index=4, title=None, uri=None)]), finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude of Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:01]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:01]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:01]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8f04f19030>, raw=Message(id='msg_0129b6Z5n1auVKKG5dLET4GV', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=43, cache_creation_input_tokens=0, cache_read_input_tokens=0)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_0129b6Z5n1auVKKG5dLET4GV', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=43, cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:11]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:11]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:13]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8f04f1b730>, raw=Message(id='msg_bdrk_01MJWhDgJBkmkPS5CiBWVB73', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=45)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_bedrock_agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_bdrk_01MJWhDgJBkmkPS5CiBWVB73', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=45))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor params vs generation params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except a few parameters, almost all the parameters can be specified in both constructor and generation method (such as `generate_text()` method). Constructor params in `Agent` class can be overriden by the arguments in `generate_text` method (as well as `stream_text(_async)` method as shown later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:22]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello!')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:22]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:23]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Hello there! How can I help you today?')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='Hello there! How can I help you today?')], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed97ef9d0>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello there! How can I help you today?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=11, prompt_token_count=3, total_token_count=14), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    temperature=0.0,\n",
    "    max_output_tokens=500,\n",
    ")\n",
    "\n",
    "agent.generate_text(prompt=\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is the same as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:24]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello!')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:24]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:25]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Hello there! How can I help you today?')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='Hello there! How can I help you today?')], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed97efdf0>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Hello there! How can I help you today?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=11, prompt_token_count=3, total_token_count=14), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    ")\n",
    "\n",
    "agent.generate_text(\n",
    "    prompt=\"Hello!\", model=\"gemini-2.0-flash-exp\", temperature=0.0, max_output_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing arguments when initiating is useful to reuse same parameters at multiple place. Especiall, multi-agent system invokes multiple agents at the same time, so it requires many parameters to be static. On the other hand, passing arguments when generating helps you control settings whenever you call the API. This is helpful for the single agent case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, here is an example to specify the same parameter when initializing agent and generating response. (As I mentioned, the later parameter is prioritized.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    n=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:27]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello! How are you?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:27]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:29]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to assist you! How can I help you today?\"), TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=\"Hello! How are you?\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to assist you! How can I help you today?\"),\n",
       " TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `n` parameter is specified in both constructor and `generate_text` method, but `n=2` is used, which was specified for `generate_text` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous, non-streaming text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate_text_async` method allows you to received text asyncronously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:36]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:36]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:37]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed8f868c0>, raw=ChatCompletion(id='chatcmpl-AlC9YPHCb4K2DHZQKKT6Xp8YyTHJ6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735811496, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0aa8d3e20b', usage=CompletionUsage(completion_tokens=30, prompt_tokens=14, total_tokens=44, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await openai_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:38]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:38]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:39]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed8416ec0>, raw=ChatCompletion(id='chatcmpl-AlC9a4DyjmAjNRfioXmqIQy4shsZ1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1735811498, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_5154047bf2', usage=CompletionUsage(completion_tokens=29, prompt_tokens=14, total_tokens=43, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await azure_openai_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:40]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:40]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:42]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed8f86650>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=CitationMetadata(citations=[Citation(end_index=139, license=None, publication_date=None, start_index=4, title=None, uri=None)]), finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await google_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:42]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:42]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:44]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed97ef310>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], role='model'), citation_metadata=CitationMetadata(citations=[Citation(end_index=139, license=None, publication_date=None, start_index=4, title=None, uri=None)]), finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await vertexai_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude of Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:44]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:44]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:45]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed83dfcd0>, raw=Message(id='msg_01BR2vjCPX5CbH9JKqG5CUqy', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=43, cache_creation_input_tokens=0, cache_read_input_tokens=0)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await claude_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude of Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:45]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:45]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:46]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed8277100>, raw=Message(id='msg_bdrk_016A2xVdv3DY6mGPGoy4q7Wf', content=[TextBlock(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=14, output_tokens=45)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await claude_bedrock_agent.generate_text_async(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous streaming text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:46]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:46]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! I'm\n",
      "Hello! I'm just\n",
      "Hello! I'm just a\n",
      "Hello! I'm just a program\n",
      "Hello! I'm just a program,\n",
      "Hello! I'm just a program, so\n",
      "Hello! I'm just a program, so I\n",
      "Hello! I'm just a program, so I don't\n",
      "Hello! I'm just a program, so I don't have\n",
      "Hello! I'm just a program, so I don't have feelings\n",
      "Hello! I'm just a program, so I don't have feelings,\n",
      "Hello! I'm just a program, so I don't have feelings, but\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you.\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:47]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = openai_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chunk are the combination of the current and past chunks, and `stream_text` method is designed to return the result finally same as the response from the `generate_text` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed8676d70>, raw=<openai.Stream object at 0x7f8ed8ffaad0>, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('root', Usage(model_name='gpt-4o-mini-2024-07-18', prompt_tokens=14, output_tokens=29))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.usage.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other client has the same interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:50]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:50]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! I'm\n",
      "Hello! I'm just\n",
      "Hello! I'm just a\n",
      "Hello! I'm just a program\n",
      "Hello! I'm just a program,\n",
      "Hello! I'm just a program, so\n",
      "Hello! I'm just a program, so I\n",
      "Hello! I'm just a program, so I don't\n",
      "Hello! I'm just a program, so I don't have\n",
      "Hello! I'm just a program, so I don't have feelings\n",
      "Hello! I'm just a program, so I don't have feelings,\n",
      "Hello! I'm just a program, so I don't have feelings, but\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you.\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:51]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = azure_openai_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed83dd9c0>, raw=<openai.Stream object at 0x7f8ed8f87af0>, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on Google AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:53]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:53]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "I'm doing well, thank you for asking! As a large language model,\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamed_response = google_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed82759f0>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='m functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n')], role='model'), citation_metadata=CitationMetadata(citations=[Citation(end_index=139, license=None, publication_date=None, start_index=4, title=None, uri=None)]), finish_message=None, token_count=None, avg_logprobs=None, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=8, total_token_count=66), automatic_function_calling_history=None, parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:56]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:56]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "I'm doing well, thank you for asking! As a large language model,\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n",
      "I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamed_response = vertexai_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"I'm doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed83df670>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='m functioning optimally and ready to assist you. How are you doing today? Is there anything I can help you with?\\n')], role='model'), citation_metadata=CitationMetadata(citations=[Citation(end_index=139, license=None, publication_date=None, start_index=4, title=None, uri=None)]), finish_message=None, token_count=None, avg_logprobs=None, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=58, prompt_token_count=7, total_token_count=65), automatic_function_calling_history=None, parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:51:59]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:51:59]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As\n",
      "Hello! As an AI language model, I don't have feelings\n",
      "Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist\n",
      "Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have.\n",
      "Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:00]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = claude_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings, but I'm functioning well and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed8292140>, raw=None, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:02]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:02]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! As\n",
      "Hello! As an\n",
      "Hello! As an AI\n",
      "Hello! As an AI language\n",
      "Hello! As an AI language model\n",
      "Hello! As an AI language model,\n",
      "Hello! As an AI language model, I\n",
      "Hello! As an AI language model, I don\n",
      "Hello! As an AI language model, I don't\n",
      "Hello! As an AI language model, I don't have\n",
      "Hello! As an AI language model, I don't have feelings\n",
      "Hello! As an AI language model, I don't have feelings or\n",
      "Hello! As an AI language model, I don't have feelings or emotions\n",
      "Hello! As an AI language model, I don't have feelings or emotions,\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly an\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have.\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today\n",
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:04]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = claude_bedrock_agent.stream_text(prompt=prompt1)\n",
    "\n",
    "for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! As an AI language model, I don't have feelings or emotions, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed8291d20>, raw=None, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronously streaming text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stream_text_async` method is responsible for asynchronous streaming text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:08]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:08]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello!\n",
      "Hello! I'm\n",
      "Hello! I'm just\n",
      "Hello! I'm just a\n",
      "Hello! I'm just a program\n",
      "Hello! I'm just a program,\n",
      "Hello! I'm just a program, so\n",
      "Hello! I'm just a program, so I\n",
      "Hello! I'm just a program, so I don't\n",
      "Hello! I'm just a program, so I don't have\n",
      "Hello! I'm just a program, so I don't have feelings\n",
      "Hello! I'm just a program, so I don't have feelings,\n",
      "Hello! I'm just a program, so I don't have feelings, but\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you.\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today\n",
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:08]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "streamed_response = openai_agent.stream_text_async(prompt=prompt1)\n",
    "\n",
    "async for chunk in streamed_response:\n",
    "    print(chunk.contents[0].text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed86747c0>, raw=<openai.AsyncStream object at 0x7f8ed8417c10>, name=None, is_last_chunk=True, prompt=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as other client, so skip demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In langrila, every client accepts `system_instruction` argument. This argument can be specified in both constructor and `generate_text(_async)`/`stream_text(_async)` method. The later case overrides constructor's system_instruction. System instruction needs to be `SystemPrompt` instance because OpenAI API needs role property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import SystemPrompt\n",
    "\n",
    "system_prompt = SystemPrompt(\n",
    "    role=\"system\",  # or developer role used for the models later than o1 for OpenAI API.\n",
    "    contents=\"You must to answer the question in Japanese.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:12]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:12]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:12]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='')], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed8291a80>, raw=ChatCompletion(id='chatcmpl-AlCA7T2DgPefqnHmrAq5u7uka52yj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735811531, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0aa8d3e20b', usage=CompletionUsage(completion_tokens=15, prompt_tokens=27, total_tokens=42, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'system', 'content': 'You must to answer the question in Japanese.'}, {'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:13]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:13]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:14]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='')], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed8417a30>, raw=ChatCompletion(id='chatcmpl-AlCA94mjPzp94NCYlUkxvKnQ1V8H2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1735811533, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_5154047bf2', usage=CompletionUsage(completion_tokens=14, prompt_tokens=27, total_tokens=41, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]), name=None, is_last_chunk=None, prompt=[{'role': 'system', 'content': 'You must to answer the question in Japanese.'}, {'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = azure_openai_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on Google AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:15]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:15]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:16]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='')], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed83dfbb0>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category='HARM_CATEGORY_HATE_SPEECH', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_DANGEROUS_CONTENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_HARASSMENT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category='HARM_CATEGORY_SEXUALLY_EXPLICIT', probability='NEGLIGIBLE', probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=7, prompt_token_count=18, total_token_count=25), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = google_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini on VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:17]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:17]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:18]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='')], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed83df880>, raw=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=inf, finish_reason='STOP', grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=7, prompt_token_count=16, total_token_count=23), automatic_function_calling_history=[], parsed=None), name=None, is_last_chunk=None, prompt=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = vertexai_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:19]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:19]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:19]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='')], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed8676f20>, raw=Message(id='msg_01MkB8GDhp6MiQqDHUh8exAf', content=[TextBlock(text='', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=23, output_tokens=28, cache_creation_input_tokens=0, cache_read_input_tokens=0)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude on Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:20]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:20]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:22]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Assistant')]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text='Assistant')], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed834bdf0>, raw=Message(id='msg_bdrk_01SNj5SnaHuF68hhWWgNu8Bm', content=[TextBlock(text='Assistant', type='text')], model='claude-3-sonnet-20240229', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=23, output_tokens=63)), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}]}])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = claude_bedrock_agent.generate_text(prompt=prompt1, system_instruction=system_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response schema using tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response schema in langrila is just tool calling. Generated args are validated with pydantic, then is finally output as text response. This feature is supported by the `response_schema_as_tool` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Language(str, Enum):\n",
    "    japanese = \"Japanese\"\n",
    "    english = \"English\"\n",
    "    french = \"French\"\n",
    "    german = \"German\"\n",
    "\n",
    "\n",
    "class GreetingTime(str, Enum):\n",
    "    morning = \"morning\"\n",
    "    afternoon = \"afternoon\"\n",
    "    evening = \"evening\"\n",
    "\n",
    "\n",
    "class Greeting(BaseModel):\n",
    "    language: Language = Field(..., title=\"Language\", description=\"Language to greet in.\")\n",
    "    greeting: str = Field(..., title=\"Greeting\", description=\"Greeting message.\")\n",
    "    time: GreetingTime = Field(..., title=\"Time\", description=\"Time of the day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:25]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:25]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:26]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Let\\'s go with Japanese. \\n\\nGreeting message: \"\" (Konnichiwa)  \\nTime of the day: \"afternoon\"')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:26]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text=\"Decide the next action based on the conversation. If you have all information for answering the user, run 'final_result' tool. If you need more information, invoke other tool to get necessary information. \")]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:26]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:26]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [ToolCallResponse(name='final_answer', args='{\"language\":\"Japanese\",\"greeting\":\"\",\"time\":\"afternoon\"}', call_id='call_kn8eLCP2eZ8qN4lSA4L3JLFs')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:26]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:26]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:26]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"language\": \"Japanese\", \"greeting\": \"\", \"time\": \"afternoon\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\n",
    "tell me the greeting message and the time of the day to use it.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    response_schema_as_tool=Greeting,  # <- This is the arg to use the schema as a tool.\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.japanese: 'Japanese'>, greeting='', time=<GreetingTime.afternoon: 'afternoon'>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`response_schema_as_tool` argument can be specified in generation method as well. If you pass this argument in both constructor and generation method, the later is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:31]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:31]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:32]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Let\\'s go with Japanese. \\n\\nA common greeting in Japanese is \"\" (Ohayou gozaimasu), which means \"Good morning.\" You would use this greeting in the morning.')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:32]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text=\"Decide the next action based on the conversation. If you have all information for answering the user, run 'final_result' tool. If you need more information, invoke other tool to get necessary information. \")]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:32]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:32]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [ToolCallResponse(name='final_answer', args='{\"language\":\"Japanese\",\"greeting\":\"\",\"time\":\"morning\"}', call_id='call_B8IREkgMu52rV3qTjcznfDR7')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:32]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:32]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:32]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"language\": \"Japanese\", \"greeting\": \"\", \"time\": \"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\n",
    "tell me the greeting message and the time of the day to use it.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt, response_schema_as_tool=Greeting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.japanese: 'Japanese'>, greeting='', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you specify the response schema when instantiating the agent, response schema is reset at every execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:34]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:34]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:38]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='I\\'ll choose French. \\n\\nA common greeting in French is \"Bonjour,\" which means \"Hello\" or \"Good morning.\" You would typically use \"Bonjour\" from the morning until the late afternoon, around 6 PM. After that time, you can switch to \"Bonsoir,\" which means \"Good evening.\"')]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll choose French. \n",
      "\n",
      "A common greeting in French is \"Bonjour,\" which means \"Hello\" or \"Good morning.\" You would typically use \"Bonjour\" from the morning until the late afternoon, around 6 PM. After that time, you can switch to \"Bonsoir,\" which means \"Good evening.\"\n"
     ]
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=prompt)\n",
    "\n",
    "print(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response schema as a tool is named `final_answer`, and its default description is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final answer which ends this conversation. Arguments of this tool must be selected from the conversation history.\n",
      "Unkown argument in the entire conversation history must be null, however, the argument appeared in the previous conversation must be provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langrila import AgentConfig\n",
    "\n",
    "agent_config = AgentConfig()\n",
    "print(agent_config.final_answer_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter is configurable, so if you want to use another description, please change the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config = AgentConfig(\n",
    "    final_answer_description=\"This is example of final answer description.\",\n",
    ")\n",
    "\n",
    "_agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    response_schema_as_tool=Greeting,\n",
    "    agent_config=agent_config,  # Set the agent configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:52]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:52]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:53]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='Please specify the language and the time of the day for the greeting.')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:53]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text=\"Decide the next action based on the conversation. If you have all information for answering the user, run 'final_result' tool. If you need more information, invoke other tool to get necessary information. \")]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:53]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:54]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [ToolCallResponse(name='final_answer', args='{\"greeting\": \"Good morning\", \"language\": \"English\", \"time\": \"morning\"}', call_id='zJgfxc3xJSyShpxFKcctmLtX')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:54]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:54]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:54]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"greeting\": \"Good morning\", \"language\": \"English\", \"time\": \"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    response_schema_as_tool=Greeting,\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.english: 'English'>, greeting='Good morning', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    client=anthropic_client,\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=500,\n",
    "    response_schema_as_tool=Greeting,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:52:58]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:52:58]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:53:03]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Certainly! I'd be happy to provide you with a greeting in one of the languages you mentioned. Let me choose a language and provide you with a greeting along with the appropriate time of day to use it.\\n\\nI'll select French for this example. Here's a common French greeting along with the time of day it's typically used:\"), ToolCallResponse(name='final_answer', args='{\"language\": \"French\", \"greeting\": \"Bonjour!\", \"time\": \"morning\"}', call_id='toolu_01CVLh2xierebps75uzcy6yp')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:53:03]\u001b[0m \u001b[1mINFO | Running tool: final_answer\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:53:03]\u001b[0m \u001b[1mINFO | Tool: final_answer successfully ran.\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:53:03]\u001b[0m \u001b[34m\u001b[1mDEBUG | Final result: [TextResponse(text='{\"language\": \"French\", \"greeting\": \"Bonjour!\", \"time\": \"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.french: 'French'>, greeting='Bonjour!', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response schema natively supported by each provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except a few arguments, Agent module accepts almost all parameters natively supported by each provider as it is. For example, if you want to use native response schema for OpenAI API or Gemini, you can specify that parameter supported by each API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:53:03]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:53:03]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:53:07]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='{\"language\":\"French\",\"greeting\":\"Bonjour\",\"time\":\"morning\"}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\n",
    "tell me the greeting message and the time of the day to use it.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    response_format=Greeting,  # native parameter for structured output\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.french: 'French'>, greeting='Bonjour', time=<GreetingTime.morning: 'morning'>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:53:10]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Tell me how to greet in a language. Please pick one language from Japanese, English, French, and German, then\\ntell me the greeting message and the time of the day to use it.\\n')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:53:10]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:53:12]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text='{\\n  \"greeting\": \"Konnichiwa\",\\n  \"language\": \"Japanese\",\\n  \"time\": \"afternoon\"\\n}')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Gemini can't support respones schema includes $defs and $ref so we need to flatten the schema.\n",
    "# Also parameter type must be upper case.\n",
    "\n",
    "from langrila.google.gemini_utils import to_gemini_schema\n",
    "\n",
    "agent = Agent(\n",
    "    client=google_dev_client,\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=to_gemini_schema(Greeting),  # native parameter for structured output\n",
    ")\n",
    "\n",
    "response = agent.generate_text(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Greeting(language=<Language.japanese: 'Japanese'>, greeting='Konnichiwa', time=<GreetingTime.afternoon: 'afternoon'>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Greeting.model_validate_json(response.contents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that different provider has the different specification and limitation of the native response schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize internal prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some agent framework uses internal prompt that is not customizable. This might affect the agent behaviour using another language like Japanese. Agent in langrila has internal prompts as well, for example, that is used for validation error retry, but that is customizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import InternalPrompt\n",
    "\n",
    "internal_prompt = InternalPrompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the internal prompts are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error_retry': 'Please fix the error on tool use. If the validation error is raised, reflect the conversation history and try to find the correct answer. If there is no fact to answer the user, try to run other tools to get necessary information. ',\n",
       " 'no_tool_use_retry': \"Decide the next action based on the conversation. If you have all information for answering the user, run 'final_result' tool. If you need more information, invoke other tool to get necessary information. \",\n",
       " 'planning': 'Please make a concise plan to answer the following question/requirement, considering the conversation history.\\nYou can invoke the sub-agents or tools to answer the questions/requirements shown in the capabilities section.\\nAgent has no description while the tools have a description.\\n\\nQuestion/Requirement:\\n{user_input}\\n\\nCapabilities:\\n{capabilities}',\n",
       " 'do_plan': 'Put the plan into action.'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_prompt.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to customize internal prompt, you can configure it via `AgentConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila import AgentConfig\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    internal_prompt=InternalPrompt(\n",
    "        error_retry=\"\",  # replace retry prompt with Japanese\n",
    "    ),\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    agent_config=agent_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langrila supports o1-family for OpenAI API. (Precisely, langrila doesn't depend on the model name.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    client=openai_client,\n",
    "    model=\"o1-mini-2024-09-12\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-01-02 18:53:21]\u001b[0m \u001b[34m\u001b[1mDEBUG | Prompt: [TextPrompt(text='Hello. How are you today?')]\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:53:21]\u001b[0m \u001b[1mINFO | root: Generating text\u001b[0m\n",
      "\u001b[32m[2025-01-02 18:53:23]\u001b[0m \u001b[34m\u001b[1mDEBUG | Response: [TextResponse(text=\"Hello! I'm doing well, thank you for asking. How can I help you today?\")]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(type='Response', role='assistant', contents=[TextResponse(text=\"Hello! I'm doing well, thank you for asking. How can I help you today?\")], usage=<langrila.core.usage.NamedUsage object at 0x7f8ed82a5f00>, raw=ChatCompletion(id='chatcmpl-AlCBEb3H1RcNZbrcFXxOFl2IIh9E6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm doing well, thank you for asking. How can I help you today?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735811600, model='o1-mini-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_e45605060d', usage=CompletionUsage(completion_tokens=158, prompt_tokens=14, total_tokens=172, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=128, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), name=None, is_last_chunk=None, prompt=[{'role': 'user', 'content': [{'text': 'Hello. How are you today?', 'type': 'text'}], 'name': None}])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.generate_text(prompt=prompt1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('root', Usage(model_name='o1-mini-2024-09-12', prompt_tokens=14, output_tokens=158))])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
