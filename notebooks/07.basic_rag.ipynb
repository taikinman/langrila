{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"NONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure your environmental variables and dependencies are ready to use LLM services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env_api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show you the examples using OpenAI embedding API. Gemini embedding API is also available in the same manner. See [embedding_text.ipynb](./06.embedding_text.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila.openai import OpenAIEmbeddingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_openai = OpenAIEmbeddingModule(\n",
    "    api_key_env_name=\"API_KEY\",\n",
    "    model_name=\"text-embedding-3-small\",\n",
    "    dimensions=1536,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is sample documents and metadatas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models.\",\n",
    "    \"LlamaIndex (GPT Index) is a data framework for your LLM application.\",\n",
    "]\n",
    "\n",
    "# [Optional] metadatas\n",
    "metadatas = [\n",
    "    {\"github_url\": \"https://github.com/taikinman/langrila\"},\n",
    "    {\"github_url\": \"https://github.com/langchain-ai/langchain\"},\n",
    "    {\"github_url\": \"https://github.com/run-llama/llama_index\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create collection at the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create collection sample_collection.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "from langrila.database.qdrant import QdrantLocalCollectionModule\n",
    "\n",
    "collection = QdrantLocalCollectionModule(\n",
    "    persistence_directory=\"./qdrant_test\",\n",
    "    collection_name=\"sample_collection\",\n",
    "    embedder=embedder_openai,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=1536,\n",
    "        distance=models.Distance.COSINE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create collection\n",
    "collection.run(\n",
    "    documents=documents, metadatas=metadatas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate retrieval module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = collection.as_retriever(\n",
    "    n_results=2,\n",
    "    score_threshold=0.3,  # The meaning of the score depends on the distance function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initiate retrieval module directly like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langrila.database.qdrant import QdrantLocalRetrievalModule\n",
    "\n",
    "retriever = QdrantLocalRetrievalModule(\n",
    "    persistence_directory=\"./qdrant_test\",\n",
    "    collection_name=\"sample_collection\",\n",
    "    embedder=embedder_openai,\n",
    "    n_results=2,\n",
    "    score_threshold=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieve from collection sample_collection...\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Langrila?\"\n",
    "\n",
    "retrieval_results = retriever.run(query=query, filter=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return is `RetrievalResults` instance which is a pydantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalResults(ids=[0, 1], documents=['Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.', 'LangChain is a framework for developing applications powered by language models.'], metadatas=[{'github_url': 'https://github.com/taikinman/langrila', 'document': 'Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.'}, {'github_url': 'https://github.com/langchain-ai/langchain', 'document': 'LangChain is a framework for developing applications powered by language models.'}], scores=[0.5302091343925472, 0.39182931721751785], collections=['sample_collection', 'sample_collection'], usage=Usage(prompt_tokens=6, completion_tokens=0, total_tokens=6))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [0, 1],\n",
       " 'documents': ['Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.',\n",
       "  'LangChain is a framework for developing applications powered by language models.'],\n",
       " 'metadatas': [{'github_url': 'https://github.com/taikinman/langrila',\n",
       "   'document': 'Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.'},\n",
       "  {'github_url': 'https://github.com/langchain-ai/langchain',\n",
       "   'document': 'LangChain is a framework for developing applications powered by language models.'}],\n",
       " 'scores': [0.5302091343925472, 0.39182931721751785],\n",
       " 'collections': ['sample_collection', 'sample_collection'],\n",
       " 'usage': {'model_name': 'text-embedding-3-small',\n",
       "  'prompt_tokens': 6,\n",
       "  'completion_tokens': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_results.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score in the result is similarity for Qdrant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qdrant server is also supported by QdrantRemoteCollectionModule and QdrantRemoteRetrievalModule. Here is a basic example using docker which app container and qdrant container are bridged by same network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create collection sample.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "from langrila.database.qdrant import QdrantRemoteCollectionModule\n",
    "\n",
    "#######################\n",
    "# create collection\n",
    "#######################\n",
    "\n",
    "collection = QdrantRemoteCollectionModule(\n",
    "    url=\"http://qdrant\",\n",
    "    port=\"6333\",\n",
    "    collection_name=\"sample\",\n",
    "    embedder=embedder_openai,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=1536,\n",
    "        distance=models.Distance.COSINE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "await collection.arun(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = collection.as_retriever(\n",
    "    n_results=2,\n",
    "    score_threshold=0.3,  # The meaning of the score depends on the distance function\n",
    ")\n",
    "\n",
    "\n",
    "# You can also use the retrieval module directly\n",
    "# from langrila.database.qdrant import QdrantRemoteRetrievalModule\n",
    "\n",
    "# retriever = QdrantRemoteRetrievalModule(\n",
    "#     url=\"http://qdrant\",\n",
    "#     port=\"6333\",\n",
    "#     collection_name=\"sample\",\n",
    "#     embedder=embedder_openai,\n",
    "#     n_results=2,\n",
    "#     score_threshold=0.3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieve from collection sample...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RetrievalResults(ids=[0, 1], documents=['Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.', 'LangChain is a framework for developing applications powered by language models.'], metadatas=[{'github_url': 'https://github.com/taikinman/langrila', 'document': 'Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.'}, {'github_url': 'https://github.com/langchain-ai/langchain', 'document': 'LangChain is a framework for developing applications powered by language models.'}], scores=[0.5301694, 0.3918205], collections=['sample', 'sample'], usage=Usage(prompt_tokens=6, completion_tokens=0, total_tokens=6))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Langrila?\"\n",
    "\n",
    "retrieval_results = await retriever.arun(query=query, filter=None)\n",
    "\n",
    "retrieval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [0, 1],\n",
       " 'documents': ['Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.',\n",
       "  'LangChain is a framework for developing applications powered by language models.'],\n",
       " 'metadatas': [{'github_url': 'https://github.com/taikinman/langrila',\n",
       "   'document': 'Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.'},\n",
       "  {'github_url': 'https://github.com/langchain-ai/langchain',\n",
       "   'document': 'LangChain is a framework for developing applications powered by language models.'}],\n",
       " 'scores': [0.5301694, 0.3918205],\n",
       " 'collections': ['sample', 'sample'],\n",
       " 'usage': {'model_name': 'text-embedding-3-small',\n",
       "  'prompt_tokens': 6,\n",
       "  'completion_tokens': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_results.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CollectionModule and RetrievalModule for chroma has the almost same interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create collection sample.\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Retrieve from collection sample...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': [0, 1],\n",
       " 'documents': ['Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.',\n",
       "  'LangChain is a framework for developing applications powered by language models.'],\n",
       " 'metadatas': [{'document': 'Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.',\n",
       "   'github_url': 'https://github.com/taikinman/langrila'},\n",
       "  {'document': 'LangChain is a framework for developing applications powered by language models.',\n",
       "   'github_url': 'https://github.com/langchain-ai/langchain'}],\n",
       " 'scores': [0.4698306096647241, 0.6081795317291985],\n",
       " 'collections': ['sample', 'sample'],\n",
       " 'usage': {'model_name': 'text-embedding-3-small',\n",
       "  'prompt_tokens': 6,\n",
       "  'completion_tokens': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langrila.database.chroma import ChromaLocalCollectionModule, ChromaLocalRetrievalModule\n",
    "\n",
    "#######################\n",
    "# create collection\n",
    "#######################\n",
    "\n",
    "collection = ChromaLocalCollectionModule(\n",
    "    persistence_directory=\"./chroma_test\",\n",
    "    collection_name=\"sample\",\n",
    "    embedder=embedder_openai,\n",
    ")\n",
    "\n",
    "collection.run(documents=documents, metadatas=metadatas) # metadatas could also be used\n",
    "\n",
    "# #######################\n",
    "# # retrieval\n",
    "# #######################\n",
    "\n",
    "# In the case collection was already instantiated\n",
    "retriever = collection.as_retriever(n_results=2, score_threshold=0.7)\n",
    "\n",
    "# # In the case collection was not instantiated\n",
    "# retriever = ChromaLocalRetrievalModule(\n",
    "#     embedder=embedder_openai,\n",
    "#     persistence_directory=\"./chroma_test\",\n",
    "#     collection_name=\"sample\",\n",
    "#     n_results=2,\n",
    "#     score_threshold=0.7,\n",
    "# )\n",
    "\n",
    "query = \"What is Langrila?\"\n",
    "retrieval_result = retriever.run(query, filter=None)\n",
    "\n",
    "# show result\n",
    "retrieval_result.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be careful for the score because chromadb retrieve index records based on distance in default, so the `scores` in `RetrievalResults` is the distance instead of similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HttpClient is also supported by `ChromaRemoteCollectionModule` and `ChromaRemoteRetrievalModule`. Here is a basic example using local server running with `chroma run` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create collection chroma_sample.\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from langrila.database.chroma import ChromaRemoteCollectionModule\n",
    "\n",
    "#######################\n",
    "# create collection\n",
    "#######################\n",
    "\n",
    "collection = ChromaRemoteCollectionModule(\n",
    "    host=\"localhost\",\n",
    "    port=\"8000\",\n",
    "    collection_name=\"chroma_sample\",\n",
    "    embedder=embedder_openai,\n",
    ")\n",
    "\n",
    "await collection.arun(documents=documents, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = collection.as_retriever(n_results=2, score_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieve from collection chroma_sample...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': [0, 1],\n",
       " 'documents': ['Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.',\n",
       "  'LangChain is a framework for developing applications powered by language models.'],\n",
       " 'metadatas': [{'document': 'Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.',\n",
       "   'github_url': 'https://github.com/taikinman/langrila'},\n",
       "  {'document': 'LangChain is a framework for developing applications powered by language models.',\n",
       "   'github_url': 'https://github.com/langchain-ai/langchain'}],\n",
       " 'scores': [0.4697908339701269, 0.6081706594022143],\n",
       " 'collections': ['chroma_sample', 'chroma_sample'],\n",
       " 'usage': {'model_name': 'text-embedding-3-small',\n",
       "  'prompt_tokens': 6,\n",
       "  'completion_tokens': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Langrila?\"\n",
    "\n",
    "retrieval_result = await retriever.arun(query, filter=None)\n",
    "retrieval_result.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For usearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usearch is a smaller & faster single-file similarity search & clustering engine for vectors. This vector db doesn't support metadata storing, so SQLite db is automatically created to store metadatas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from langrila.database.usearch import UsearchLocalCollectionModule, UsearchLocalRetrievalModule\n",
    "\n",
    "#######################\n",
    "# create collection\n",
    "#######################\n",
    "\n",
    "collection = UsearchLocalCollectionModule(\n",
    "    persistence_directory=\"./usearch_test\",\n",
    "    collection_name=\"sample\",\n",
    "    embedder=embedder_openai,\n",
    "    dtype = \"f16\",\n",
    "    ndim = 1536,\n",
    "    connectivity = 16,\n",
    "    expansion_add = 128,\n",
    "    expansion_search = 64,\n",
    ")\n",
    "\n",
    "# Strongly recommended because search result may be different when new vectors are inserted after existing vectors are removed.\n",
    "# Instead, rebuilding the index is recommended using `delete_collection` before upserting.\n",
    "# Or use exact search to avoid this issue when search time.\n",
    "collection.delete_collection()\n",
    "\n",
    "collection.run(documents=documents) # metadatas could also be used. \n",
    "\n",
    "# #######################\n",
    "# # retrieval\n",
    "# #######################\n",
    "\n",
    "# In the case collection was already instantiated\n",
    "retriever = collection.as_retriever(n_results=2, score_threshold=0.7)\n",
    "\n",
    "# # In the case collection was not instantiated\n",
    "# retriever = UsearchLocalRetrievalModule(\n",
    "#     embedder=embedder_openai,\n",
    "#     persistence_directory=\"./usearch_test\",\n",
    "#     collection_name=\"sample\",\n",
    "#     dtype = \"f16\",\n",
    "#     ndim=1536,\n",
    "#     connectivity = 16,\n",
    "#     expansion_add = 128,\n",
    "#     expansion_search = 64,\n",
    "#     n_results=2,\n",
    "#     score_threshold=0.5,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieve from collection sample...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': [0, 1],\n",
       " 'documents': ['Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.',\n",
       "  'LangChain is a framework for developing applications powered by language models.'],\n",
       " 'metadatas': [{'document': 'Langrila is a useful tool to use ChatGPT with OpenAI API or Azure in an easy way.'},\n",
       "  {'document': 'LangChain is a framework for developing applications powered by language models.'}],\n",
       " 'scores': [0.4698024392127991, 0.608174204826355],\n",
       " 'collections': ['sample', 'sample'],\n",
       " 'usage': {'model_name': 'text-embedding-3-small',\n",
       "  'prompt_tokens': 6,\n",
       "  'completion_tokens': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Langrila?\"\n",
    "retrieval_result = retriever.run(query, filter=None, exact=False)\n",
    "\n",
    "# show result\n",
    "retrieval_result.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For usearch, score is distance, not similarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
